{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd615d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Блок 1: Импорты -------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Импорт словаря перевода\n",
    "from animals_sber import animals_sber\n",
    "\n",
    "# Проверка доступности GPU и установка устройства\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# ------------------------- Блок 2: Параметры -------------------------\n",
    "\n",
    "DATASET_DIR = r'C:/Users/admin/Desktop/work/model_resnet/датасет'\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LR = 3e-5\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  \n",
    "\n",
    "# ------------------------- Блок 3: Подготовка данных -------------------------\n",
    "\n",
    "# Трансформации для данных, на train усиленная аугментация \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(DATASET_DIR)\n",
    "\n",
    "# Вывод информации о количестве изображений\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Number of classes: {len(full_dataset.classes)}\")\n",
    "print(\"Class distribution:\")\n",
    "for class_name, count in zip(full_dataset.classes, np.bincount(full_dataset.targets)):\n",
    "    print(f\"{class_name}: {count} images\")\n",
    "\n",
    "\n",
    "\n",
    "# Разделение на тренировочную, валидационную и тестовую выборки\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Собираем индексы по классам\n",
    "class_indices = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(full_dataset.samples):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Перемешиваем внутри каждого класса\n",
    "for label in class_indices:\n",
    "    random.shuffle(class_indices[label])\n",
    "\n",
    "train_idx = []\n",
    "val_idx = []\n",
    "test_idx = []\n",
    "\n",
    "for label, indices in class_indices.items():\n",
    "    n_total = len(indices)\n",
    "    n_test = int(TEST_SPLIT * n_total)\n",
    "    n_val = int(VAL_SPLIT * n_total)\n",
    "    n_train = n_total - n_test - n_val\n",
    "    \n",
    "    train_idx.extend(indices[:n_train])\n",
    "    val_idx.extend(indices[n_train:n_train + n_val])\n",
    "    test_idx.extend(indices[n_train + n_val:])\n",
    "\n",
    "random.shuffle(train_idx)\n",
    "random.shuffle(val_idx)\n",
    "random.shuffle(test_idx)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    DATASET_DIR,\n",
    "    transform=train_transform\n",
    ")\n",
    "train_dataset = Subset(train_dataset, train_idx)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    DATASET_DIR,\n",
    "    transform=val_transform\n",
    ")\n",
    "val_dataset = Subset(val_dataset, val_idx)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    DATASET_DIR,\n",
    "    transform=test_transform\n",
    ")\n",
    "test_dataset = Subset(test_dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"Training set: {len(train_dataset)} images\")\n",
    "print(f\"Validation set: {len(val_dataset)} images\")\n",
    "print(f\"Test set: {len(test_dataset)} images\")\n",
    "\n",
    "# ------------------------- Блок 4: Модель -------------------------\n",
    "\n",
    "class ResNet50Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50Model, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Замораживаем все слои\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Заменяем последний слой\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "model = ResNet50Model(num_classes).to(device)\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ------------------------- Блок 5: Callbacks и утилиты -------------------------\n",
    "\n",
    "# Ранняя остановка\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "# Сохранение модели\n",
    "def save_model(model, path='best_resnet50.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Планировщик обучения\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model, data_loader, criterion, dataset_name=\"Validation\"):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Results:\")\n",
    "    print(f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    translated_labels = [animals_sber.get(label, label) for label in class_names]\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=translated_labels))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(confusion_matrix(all_labels, all_preds), annot=True, fmt=\"d\", \n",
    "                xticklabels=translated_labels, yticklabels=translated_labels, cmap=\"Blues\")\n",
    "    plt.title(f\"{dataset_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# ------------------------- Блок 6: Обучение -------------------------\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, optimizer, criterion):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_acc = correct_train / total_train\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = correct_val / total_val\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Первый этап обучения (замороженные слои)\n",
    "print(\"\\nFirst stage training (frozen layers)\")\n",
    "model = train_model(model, train_loader, val_loader, EPOCHS, optimizer, criterion)\n",
    "\n",
    "# Оценка на валидационной выборке после первого этапа\n",
    "print(\"\\nEvaluating on validation set after first stage:\")\n",
    "val_preds, val_labels = evaluate_model(model, val_loader, criterion, \"Validation\")\n",
    "\n",
    "# ------------------------- Блок 7: Разморозка и дообучение -------------------------\n",
    "\n",
    "# Размораживаем все слои\n",
    "for param in model.resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Новый оптимизатор с меньшим learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR/10)\n",
    "\n",
    "# Второй этап обучения\n",
    "print(\"\\nSecond stage training (unfrozen layers)\")\n",
    "model = train_model(model, train_loader, val_loader, 20, optimizer, criterion)\n",
    "\n",
    "# Оценка на валидационной выборке после второго этапа\n",
    "print(\"\\nEvaluating on validation set after second stage:\")\n",
    "val_preds, val_labels = evaluate_model(model, val_loader, criterion, \"Validation\")\n",
    "\n",
    "# ------------------------- Блок 8: Оценка на тестовой выборке -------------------------\n",
    "\n",
    "# Загрузка лучшей модели\n",
    "model.load_state_dict(torch.load('best_resnet50.pth', map_location=device))\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "print(\"\\nFinal evaluation on test set:\")\n",
    "test_preds, test_labels = evaluate_model(model, test_loader, criterion, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
