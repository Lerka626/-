# -
 классификация животных 
Давайте пробежимся по блокам:

1) Импорты. Импорты библиотек думаю объяснять не нужно, так же импортируется файл translate.py представленный вместе с датасетом, в нем хранится словарь состоящий из ключа (названия папки) и значения (названия животных на фото в папке). Так же определяем устройство, на котором будет происходить обучение (CPU or GPU). Предпочтительно использовать GPU, тк на нем обучение происходит быстрее.

2) Параметры. Указываем путь к ключевой папке, в которой хранятся папки с изображениями. Сама структура датасета выстроена таким образом:
Папка "new_datasets"
|--Папка "raw-img"
|        |--Папки с различными названиями, в которых хранятся изображения (в данном случе 10 папок)
|--Файл "translate.py"
Так же ссылка на датасет: https://www.kaggle.com/datasets/alessiocorrado99/animals10
После мы задаем параметры:
-IMG_SIZE: уже понятно из названия, что за параметр. 224x224 пикселей
-BATCH_SIZE: количество изображений, обрабатываемых за 1 шаг. В моем случае можно было поставить даже 64 благодаря мощности компьютера (это для видеокарты RTX 3070). 
-EPOCHS: задаем количество эпох. 1 эпоха - 1 проход по тренировочному набору данных, слишком сильно повышать этот параметр не стоит, тк модель может переобучиться. Как только значения accuracy и loss перестают улучшаться, переходя к следующей эпохе - это сигнал о переобучении модели.
-LR: скорость обучения, я взяла среднюю (не большая и не маленькая)
-VAL_SPLIT: доля валидации. В моем случае датасет делится в соотношении 80/20, где 80% датасета предназначено для обучения модели, а 20% для ее тестирования.

3) Подготовка данных. В train_transform происходит: изменение размера, поворот изображения, случайное отражение, случайное кадрирование, преобразование в тензор и нормализация пикселей. В ней происходят такие трансформации для избежания переобучения (тк могут быть одинаковые или почти одинаковые картинки) и помогает обучить модель справляться с определением животного в разных условия, например, когда изображение обрезано и видна только часть животного (например, половина тела). В val_transform - это изменение размера, преобразование в тензор и нормализация. Здесь таких изменений не происходит, тк эти изображения предназначены для проверки модели на чистых изображениях.
в full_datasets загружается весь датасет; в train_idx и val_idx происходит разделение на тестовую и валидационную выборки. Первые два параметра, думаю, ясны (создание списка индексов и разделение в соотношении 80/20), параметр stratify позволяет равномерно распределить классы по соотношениям, а random_state фиксирует случайность.
train_dataset - загрузка изображений из папки DATASET_DIR и применение трансформации для тренировочных данных. После чего остаются только индексы. (чтоб не читать повторно с диска)
val_dataset - то же самое, только изображения не изменяются (нет отражений, поворотов и тд).
train_loader, val_loader - загрузчики данных, для эффективной передачи данных в модель. Значения параметров практически одинаковые, потому разберем их вместе:
-train_dataset - передаем индексы
-batch_size - количество обрабатываемых изображений за итерацию
-shuffle - True означает перемешивание данных, чтоб модель не запоминала их порядок, False - наоборот
-num_workers - количество потоков для загрузки данных, что ускоряет процесс
-pin_memory - ускоряет передачу данных на GPU при использовании CUDA.
Далее получаем информацию о классах. В class_names получаем названия классов, а в num_class получаем количество классов.

4) Модель. Создаем класс модели ResNet50Model, где в объект класса загружается модель ResNet, предобученная на ImageNet; после этого происходит заморозка слоев, чтоб не дообучать модель, то есть не меняем веса, а используем их как есть, если хотите обратного, то просто в цикле for передайте переменной param.requires_grad значение True вместо False. Потом заменяем последний слой для того, чтоб он соответствовал числу классов в нашем датасете, так же задаем метод forward, который осуществляет передачу данных в модель. После в переменную model мы создаем экземпляр модели и отправляем его на наше устройство (в моем случае GPU). В criterion задается функция потерь, а в optimizer задается оптимизатор Adam, который изменяет веса модели, для уменьшения ошибки loss на каждой эпохе.

5) Callbacks и утилиты. Создаем класс ранней остановки, в объекте которого задаются переменные:
-patience - количество эпох без улучшений перед остановкой
-delta - минимальное улучшение, чтоб эпоха не считалась бесполезной
-counter - счетчик количества эпох без улучшений
-best_score - модель с минимальной ошибкой
-early_stop - влаг остановки обучения
Так же задается метод call, в котором как раз таки задается парядок действий: сохранение лучшей модели, либо увеличение счетчика. В переменную early_stopping задаем экземпляр ранней остановки. Так же есть отдельно от класса функция по сохранению модели (save_model) и есть планировщик скорости обучения scheduler, который автоматически скизит скорость обучения, если модель перестала улучшаться.

6) Создаем функцию train_model и передаем в нее модель, подготовленные данные, параметры и созданная функция потерь и оптимизатор. Создается переменная best_val_loss для созранения лучшей ошибки, после начинаем цикл по эпохам, в котором тренируем модель, задются параметры подсчета ошибки и точности, после чего сразу заходим в следующий цикл for, в котором получаем картинки и их классы и передаем их на GPU, обнуляем градиенты, получаем предсказания, считаем функцию потерь, выполняем обратное распространение ошибки и обновляем веса модели. Выйдя из второго цикла for, оставаясь в первом цикле мы считаем среднюю ошибку и точность; после переходим к валидации, то есть переключаем модель в режим оценки и вычисляем предсказания, потери и точность, сохраняем все предсказания. К окончанию цикла обновляем скорость обучения, праверяем нужно ли сделать раннюю остановку и сохраняем лучшую модель. Сама функция возвращает предсказания и истинные значения. Вне функции начинаем первый этап обучения на новых слоях (предобученные не трогаем).

7) Разморозка и дообучение. Сначала в цикле for размораживаем все слои, передав значение True в переменную param.requires_grad, задаем новый оптимизатор с меньшей скоростью и начинаем второй этап обучения с сохранением предсказаний и истинных значений.

8) Оценка. Загружаем лучшую модель и переводим ее в режим оценки, переводим намера в названия классов, выводим отчет классификации и строим матрицу ошибок.
